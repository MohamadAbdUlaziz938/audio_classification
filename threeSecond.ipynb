{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA & MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathAudioHalf = 'D:/AudioIRA/dataset7.new.0.5/'\n",
    "\n",
    "pathAudioOne = 'D:/AudioIRA/dataset7.new.1/'\n",
    "\n",
    "pathAudioThree = 'D:/AudioIRA/dataset7.new.3/'\n",
    "\n",
    "pathAudioFive = 'D:/AudioIRA/dataset7.new..5/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import IPython.display as ipd\n",
    "from pywt import wavedec\n",
    "import pywt\n",
    "import copy\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import random\n",
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pydub import AudioSegment\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import warnings\n",
    "from glob import glob\n",
    "from librosa import feature\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "# Libraries for Classification and building Models\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions For Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "    # ----------------------------\n",
    "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sr == newsr):\n",
    "          # Nothing to do\n",
    "          return aud\n",
    "\n",
    "        num_channels = sig.shape[0]\n",
    "        # Resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1, :])\n",
    "        if (num_channels > 1):\n",
    "            # Resample the second channel and merge both channels\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:, :])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "\n",
    "        return ((resig, newsr))\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sig.shape[0] == new_channel):\n",
    "          # Nothing to do\n",
    "          return aud\n",
    "\n",
    "        if (new_channel == 1):\n",
    "          # Convert from stereo to mono by selecting only the first channel\n",
    "          resig = sig[:1, :]\n",
    "        else:\n",
    "          # Convert from mono to stereo by duplicating the first channel\n",
    "          resig = torch.cat([sig, sig])\n",
    "\n",
    "        return ((resig, sr))\n",
    "\n",
    "\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec\n",
    "\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = aud\n",
    "        top_db = 80\n",
    "\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "        # Convert to decibels\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return (spec)\n",
    "    @staticmethod\n",
    "    def apply_LowPassFilter(aud):\n",
    "        transform = Compose(\n",
    "            [LowPassFilter(sample_rate=aud[1])],\n",
    "        )\n",
    "        t_audio = transform(aud[0])\n",
    "        # torchaudio.save(\"tests/filter.wav\", t_audio, sample_rate=sample_rate)\n",
    "        assert t_audio.shape == aud[0].shape\n",
    "\n",
    "        return t_audio , aud[1]\n",
    "    @staticmethod\n",
    "    def lowpassfilter(signal, thresh = 0.63, wavelet=\"db2\",level=4):\n",
    "        thresh = thresh*np.nanmax(signal)\n",
    "        coeff = pywt.wavedec(signal, wavelet, mode=\"per\" ,level=level)\n",
    "        coeff[1:] = (pywt.threshold(i, value=thresh, mode=\"soft\" ) for i in coeff[1:])\n",
    "        reconstructed_signal = pywt.waverec(coeff, wavelet, mode=\"per\")\n",
    "        return reconstructed_signal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataFirst(num,level=2,thresh=1,augmentData=False , dwt = False , dwtMfcc = False , mfcc = False):\n",
    "    if(num == 0.5):\n",
    "        data_dir = pathAudioHalf\n",
    "        folders = glob(data_dir+'/*')\n",
    "\n",
    "        feat = []\n",
    "        lab = []\n",
    "    if(num == 1):\n",
    "        data_dir = pathAudioOne\n",
    "        folders = glob(data_dir+'/*')\n",
    "\n",
    "        feat = []\n",
    "        lab = []\n",
    "    if(num == 3):\n",
    "        data_dir = pathAudioThree\n",
    "        folders = glob(data_dir+'/*')\n",
    "\n",
    "        feat = []\n",
    "        lab = []\n",
    "    if(num == 5):\n",
    "        data_dir = pathAudioFive\n",
    "        folders = glob(data_dir+'/*')\n",
    "\n",
    "        feat = []\n",
    "        lab = []\n",
    "        \n",
    "        \n",
    "    i=-1\n",
    "    for folder in folders:\n",
    "        audio_files=glob(folder + '/*.wav')\n",
    "        i=i+1\n",
    "        for file in audio_files:\n",
    "            \n",
    "            \n",
    "            if dwtMfcc:\n",
    "                sr,signal=wavfile.read(file)\n",
    "                if len(signal.shape) == 2:\n",
    "                    signal = signal.T\n",
    "                    signal = signal[0]\n",
    "                aud2 = AudioUtil.lowpassfilter(signal,thresh = thresh,level = level)        \n",
    "                aud2 = aud2/max(aud2)\n",
    "                aud2=aud2.reshape(1,aud2.shape[0])\n",
    "                tens=torch.tensor(aud2,dtype=torch.float32)\n",
    "                aud=(tens,sr)\n",
    "\n",
    "                reaud = AudioUtil.resample(aud, 16000)\n",
    "                rechan = AudioUtil.rechannel(reaud, 2)\n",
    "                sgram = AudioUtil.spectro_gram(rechan, n_mels=64, n_fft=1024, hop_len=None).T\n",
    "                s = np.mean(np.array(sgram),axis=0)\n",
    "                s.reshape(128)\n",
    "                feat.append(s)\n",
    "                lab.append(i)\n",
    "                \n",
    "                \n",
    "                \n",
    "            if dwt:\n",
    "                sr,signal=wavfile.read(file)\n",
    "#                 print(file)\n",
    "                if len(signal.shape) == 2:\n",
    "                    signal = signal.T\n",
    "                    signal = signal[0]\n",
    "                aud2 = AudioUtil.lowpassfilter(signal,thresh = 0.01,level = 2)        \n",
    "                aud2 = aud2/max(aud2)\n",
    "                aud2=aud2.reshape(1,aud2.shape[0])\n",
    "                tens=torch.tensor(aud2,dtype=torch.float32)\n",
    "                aud=(tens,sr)\n",
    "\n",
    "                reaud = AudioUtil.resample(aud, 16000)\n",
    "                rechan = AudioUtil.rechannel(reaud, 2)\n",
    "                \n",
    "                if(len(rechan[0][0]) % 128 !=0):\n",
    "                    ss = len(rechan[0][0]) % 128\n",
    "                    j= rechan[0][0].resize_((len(rechan[0][0])+(128-ss)))\n",
    "                    k=rechan[0][1].resize_((len(rechan[0][1])+(128-ss)))\n",
    "\n",
    "                    j = np.asarray(j)\n",
    "                    res = len(j)/128\n",
    "\n",
    "                    re = j.reshape([int(res),64,2])\n",
    "                    re = np.mean(np.array(re),axis=0)\n",
    "                    re = re.reshape(128)\n",
    "                    s = re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "#                     res = rechan[0].shape[1]/128\n",
    "#                     re = rechan[0].reshape([int(res),64,2])\n",
    "                    \n",
    "                    j= rechan[0][0]\n",
    "                    j = np.asarray(j)\n",
    "                    res = len(j)/128\n",
    "                    \n",
    "                    re = j.reshape([int(res),64,2])\n",
    "                    re = np.mean(np.array(re),axis=0)\n",
    "                    re = re.reshape(128)\n",
    "                    s = re\n",
    "                    \n",
    "                    \n",
    "                feat.append(s)\n",
    "                lab.append(i)\n",
    "                \n",
    "            \n",
    "            if mfcc : \n",
    "                aud = AudioUtil.open(file)\n",
    "                reaud = AudioUtil.resample(aud, 16000)\n",
    "                rechan = AudioUtil.rechannel(reaud, 2)\n",
    "                sgram = AudioUtil.spectro_gram(rechan, n_mels=64, n_fft=1024, hop_len=None).T\n",
    "                s = np.mean(np.array(sgram),axis=0)\n",
    "                s = s.reshape(128)\n",
    "                feat.append(s)\n",
    "                lab.append(i)\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            if augmentData:\n",
    "                sgram2 = AudioUtil.spectro_gram(rechan, n_mels=64, n_fft=1024, hop_len=None)\n",
    "                aug_sgram = AudioUtil.spectro_augment(sgram2, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2).T\n",
    "                s2 = np.mean(np.array(aug_sgram),axis=0)\n",
    "                s2 = s2.reshape(128)\n",
    "                feat.append(s2)\n",
    "                lab.append(i)\n",
    "            \n",
    "                \n",
    "            \n",
    "    \n",
    "    return [feat , lab]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intiModel(X_train , X_test , Y_train , Y_test , PCA = False):\n",
    "    \n",
    "    if PCA :         \n",
    "        X_train = X_train.reshape(X_train.shape[0], 4, 8, 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], 4, 8, 1)\n",
    "\n",
    "        input_dim = (4, 8, 1)\n",
    "    else : \n",
    "        X_train = X_train.reshape(X_train.shape[0], 16, 8, 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], 16, 8, 1)\n",
    "\n",
    "        input_dim = (16, 8, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding = \"same\", activation = \"tanh\", input_shape = input_dim))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(256, (3, 3), padding = \"same\", activation = \"tanh\"))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation = \"tanh\"))\n",
    "    model.add(Dense(Y_train.shape[1], activation = \"softmax\"))\n",
    "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model , X_train , X_test ,Y_train , Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_explained_variance(X):\n",
    "    pca = PCA().fit(X)\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_projection(n_compnent,X):\n",
    "    pca = PCA(n_compnent)  # project from 64 to n_compnent dimensions\n",
    "    projected = pca.fit_transform(X)\n",
    "    print ('original data shape : ')\n",
    "    print(X.shape)\n",
    "    print ('projected data shape : ')\n",
    "    print(projected.shape)\n",
    "    return projected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## draw History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawHistory(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## draw all historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawALLHistoryAccuracy(history):\n",
    "    for i in range(5):\n",
    "        plt.plot(history[i].history['accuracy'])\n",
    "    plt.title('train model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['KFold1', 'KFold2','KFold3','KFold4','KFold5'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(5):\n",
    "        plt.plot(history[i].history['val_accuracy'])\n",
    "    plt.title('test model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['KFold1', 'KFold2','KFold3','KFold4','KFold5'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawALLHistoryLoss(history):\n",
    "    for i in range(5):\n",
    "        plt.plot(history[i].history['loss'])\n",
    "    plt.title('train model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['KFold1', 'KFold2','KFold3','KFold4','KFold5'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(5):\n",
    "        plt.plot(history[i].history['val_loss'])\n",
    "    plt.title('test model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['KFold1', 'KFold2','KFold3','KFold4','KFold5'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## draw Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawConfusionMatrix(Y_test,preds):\n",
    "    confusion_mc = confusion_matrix(Y_test, preds)\n",
    "    # put it in data frame to visualize\n",
    "    df_cm = pd.DataFrame(confusion_mc, \n",
    "                         index = [i for i in range(0,confusion_mc.shape[0])], columns = [i for i in range(0,confusion_mc.shape[0])])\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.heatmap(df_cm, annot=True,)\n",
    "    plt.title('SVM Linear Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(Y_test,preds)))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the variables model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialVariabels(num, augmentData=False ,dwt = False,mfcc=False,dwtMfcc=False):    \n",
    "    \n",
    "    #read and init data to half second model\n",
    "    temp = readDataFirst(num = num , level=1 , thresh=0.01 , augmentData = augmentData ,dwt = dwt,dwtMfcc=dwtMfcc,mfcc=mfcc)\n",
    "    temp = np.array(temp)\n",
    "    data = temp.transpose()\n",
    "    X_ = data[:, 0]\n",
    "    Y = data[:, 1]\n",
    "\n",
    "    X = np.empty([X_.shape[0], 128])\n",
    "    for i in range(X_.shape[0]):\n",
    "        X_[i] = np.reshape(X_[i] , (128))\n",
    "        X[i] = (X_[i]) \n",
    "\n",
    "    Y = to_categorical(Y)\n",
    "\n",
    "\n",
    "\n",
    "    print('All Data leangth is : {}'.format(X_.shape[0]))\n",
    "    print()\n",
    "    print('categorical leangth is : {}'.format(Y.shape[0]))\n",
    "    \n",
    "    return X , Y \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kfold Fiting Model Cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitingModelCnn(X , Y , PCA = False):\n",
    "    #scores & history & confusionMatrix & BPNN data X,Y & historyBPNN\n",
    "    scores = []\n",
    "    history = []\n",
    "    confusionMatrix = []\n",
    "    BPNN = []\n",
    "    \n",
    "    \n",
    "    cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    iKFold = 1 \n",
    "    for train_index, test_index in cv.split(X):\n",
    "        #split data to train & test  from KFOLD Split\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], Y[train_index], Y[test_index]\n",
    "        print('Train Data leangth for KFold :{} : is {}'.format(iKFold , X_train.shape[0]))\n",
    "        print('Test Data leangth for KFold :{} : is {}'.format(iKFold , X_test.shape[0]))\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "        #intializing cnn model to half second model\n",
    "        halfSecondModel, halfSecondX_train , halfSecondX_test ,halfSecondY_train ,halfSecondY_test = intiModel(X_train , X_test , y_train , y_test , PCA = PCA)\n",
    "\n",
    "        #fit model\n",
    "        historyT = halfSecondModel.fit(halfSecondX_train, halfSecondY_train, epochs = 7, batch_size = 50, validation_data = (halfSecondX_test, halfSecondY_test))\n",
    "\n",
    "        #evaluate model\n",
    "        ModelScore = halfSecondModel.evaluate(halfSecondX_test, halfSecondY_test)\n",
    "\n",
    "        #save result\n",
    "        scores.append(ModelScore[1])\n",
    "        history.append(historyT)\n",
    "\n",
    "        #save for confusionMatrix\n",
    "        predictions = halfSecondModel.predict(halfSecondX_test)\n",
    "        predictions = np.argmax(predictions, axis = 1)\n",
    "        Y_test_max = np.argmax(halfSecondY_test, axis = 1)\n",
    "        confusionMatrix.append([Y_test_max,predictions])    \n",
    "\n",
    "        #save for BPNN\n",
    "        BPNN.append([halfSecondX_train, halfSecondX_test, halfSecondY_train, halfSecondY_test])\n",
    "\n",
    "\n",
    "        iKFold = iKFold +1\n",
    "        \n",
    "    return scores, history , confusionMatrix , BPNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DWT Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showSignal(file):\n",
    "    sr,signal=wavfile.read(file);\n",
    "    (cA1, cD1) = pywt.dwt(signal, 'db2', 'smooth')\n",
    "    reconstructed_signal = pywt.idwt(cA1, cD1, 'db2', 'smooth')\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.plot(signal, label='signal')\n",
    "    ax.plot(reconstructed_signal, label='reconstructed signal', linestyle='--')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    return signal , sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMultiLevel(chirp_signal):\n",
    "    fig, ax = plt.subplots(figsize=(6,1))\n",
    "    ax.set_title(\"Original Chirp Signal: \")\n",
    "    ax.plot(chirp_signal)\n",
    "    plt.show()\n",
    "\n",
    "    data = chirp_signal\n",
    "    waveletname = 'sym5'\n",
    "\n",
    "    fig, axarr = plt.subplots(nrows=5, ncols=2, figsize=(6,6))\n",
    "    for ii in range(5):\n",
    "        (data, coeff_d) = pywt.dwt(data, waveletname)\n",
    "        axarr[ii, 0].plot(data, 'r')\n",
    "        axarr[ii, 1].plot(coeff_d, 'g')\n",
    "        axarr[ii, 0].set_ylabel(\"Level {}\".format(ii + 1), fontsize=14, rotation=90)\n",
    "        axarr[ii, 0].set_yticklabels([])\n",
    "        if ii == 0:\n",
    "            axarr[ii, 0].set_title(\"Approximation coefficients\", fontsize=14)\n",
    "            axarr[ii, 1].set_title(\"Detail coefficients\", fontsize=14)\n",
    "        axarr[ii, 1].set_yticklabels([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showDWT(signal):\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    ax.plot(signal, color=\"b\", alpha=0.5, label='original signal')\n",
    "    rec = AudioUtil.lowpassfilter(signal, thresh=0.01,level=2)\n",
    "    ax.plot(rec, 'k', label='DWT smoothing}', linewidth=1)\n",
    "    ax.legend()\n",
    "    ax.set_title('Removing High Frequency Noise with DWT', fontsize=18)\n",
    "    ax.set_ylabel('Signal Amplitude', fontsize=16)\n",
    "    ax.set_xlabel('Sample No', fontsize=16)\n",
    "    plt.show()\n",
    "    ipd.Audio(rec, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAccuracy(scores , all_history , file , isCNN ):\n",
    "    iKFold = 0\n",
    "    score = 0\n",
    "    indexMaxScore = 0\n",
    "    for sc in scores:\n",
    "        if sc > score:\n",
    "            score = sc\n",
    "            indexMaxScore = iKFold\n",
    "        iKFold = iKFold +1\n",
    "    bestScore_halfSecond = all_history[indexMaxScore]\n",
    "    \n",
    "\n",
    "    # convert the history.history dict to a pandas DataFrame:\n",
    "    if isCNN :\n",
    "        hist_df = pd.DataFrame(bestScore_halfSecond.history)\n",
    "    else : \n",
    "        hist_df = pd.DataFrame(bestScore_halfSecond.validation_scores_) \n",
    "    #save to csv: \n",
    "    #hist_csv_file = file\n",
    "    #with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(file,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show all accuracy levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN accuracy levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALLCNNAccuracyLevels(histLevel1 , histLevel2 ,title1 , title2 ,nLevel1 , nLevel2):\n",
    "    plt.plot(histLevel1['accuracy'])      \n",
    "    plt.plot(histLevel2['accuracy'])\n",
    "\n",
    "    plt.title(title1)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend([nLevel1 , nLevel2 ], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(histLevel1['val_accuracy'])      \n",
    "    plt.plot(histLevel2['val_accuracy'])\n",
    "    \n",
    "    plt.title(title2)\n",
    "    plt.ylabel('val_accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend([nLevel1 , nLevel2], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- DWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we need augment just change augmentData=True\n",
    "\n",
    "X_dwt , Y_dwt = intialVariabels(3 , augmentData=False ,dwt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_dwt, history_dwt , confusionMatrix_dwt , BPNN_dwt = fitingModelCnn(X_dwt , Y_dwt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for ev in scores_dwt:\n",
    "    print(\"this is score for :{} KFold:\".format(iKFold))\n",
    "    print(ev)\n",
    "    print()\n",
    "    iKFold+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for hist in history_dwt:\n",
    "    print(\"this is history for :{} KFold:\".format(iKFold))\n",
    "    drawHistory(hist)\n",
    "    print()\n",
    "    iKFold =iKFold + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawALLHistoryAccuracy(history_dwt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawALLHistoryLoss(history_dwt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for conv in confusionMatrix_dwt:\n",
    "    print(\"this is confusion Matrix for :{} KFold:\".format(iKFold))\n",
    "    drawConfusionMatrix(conv[0] , conv[1])\n",
    "    print()\n",
    "    iKFold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- DWT & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_explained_variance(X_dwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected_dwt=get_pca_projection(32,X_dwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_dwtPca, history_dwtPca , confusionMatrix_dwtPca , BPNN_dwtPca = fitingModelCnn(X_projected_dwt , Y_dwt , PCA = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for ev in scores_dwtPca:\n",
    "    print(\"this is score for :{} KFold:\".format(iKFold))\n",
    "    print(ev)\n",
    "    print()\n",
    "    iKFold+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for hist in history_dwtPca:\n",
    "    print(\"this is history for :{} KFold:\".format(iKFold))\n",
    "    drawHistory(hist)\n",
    "    print()\n",
    "    iKFold =iKFold + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawALLHistoryAccuracy(history_dwtPca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawALLHistoryLoss(history_dwtPca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for conv in confusionMatrix_dwtPca:\n",
    "    print(\"this is confusion Matrix for :{} KFold:\".format(iKFold))\n",
    "    drawConfusionMatrix(conv[0] , conv[1])\n",
    "    print()\n",
    "    iKFold+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- DWT & MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we need augment just change augmentData=True\n",
    "\n",
    "X_DwtMfcc , Y_DwtMfcc = intialVariabels(3 , augmentData=False ,dwtMfcc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_DwtMfcc, history_DwtMfcc , confusionMatrix_DwtMfcc , BPNN_DwtMfcc = fitingModelCnn(X_DwtMfcc , Y_DwtMfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for ev in scores_DwtMfcc:\n",
    "    print(\"this is score for :{} KFold:\".format(iKFold))\n",
    "    print(ev)\n",
    "    print()\n",
    "    iKFold+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for hist in history_DwtMfcc:\n",
    "    print(\"this is history for :{} KFold:\".format(iKFold))\n",
    "    drawHistory(hist)\n",
    "    print()\n",
    "    iKFold =iKFold + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawALLHistoryAccuracy(history_DwtMfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawALLHistoryLoss(history_DwtMfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for conv in confusionMatrix_DwtMfcc:\n",
    "    print(\"this is confusion Matrix for :{} KFold:\".format(iKFold))\n",
    "    drawConfusionMatrix(conv[0] , conv[1])\n",
    "    print()\n",
    "    iKFold+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- MFCC & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we need augment just change augmentData=True\n",
    "\n",
    "X_Mfcc , Y_Mfcc = intialVariabels(3 , augmentData=False ,mfcc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_explained_variance(X_Mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected_mfccPca=get_pca_projection(32,X_Mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_mfccPca, history_mfccPca , confusionMatrix_mfccPca , BPNN_mfccPca = fitingModelCnn(X_projected_mfccPca , Y_Mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for ev in scores_mfccPca:\n",
    "    print(\"this is score for :{} KFold:\".format(iKFold))\n",
    "    print(ev)\n",
    "    print()\n",
    "    iKFold+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for hist in history_mfccPca:\n",
    "    print(\"this is history for :{} KFold:\".format(iKFold))\n",
    "    drawHistory(hist)\n",
    "    print()\n",
    "    iKFold =iKFold + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawALLHistoryAccuracy(history_mfccPca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw all history loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawALLHistoryLoss(history_mfccPca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### draw confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iKFold =1\n",
    "for conv in confusionMatrix_mfccPca:\n",
    "    print(\"this is confusion Matrix for :{} KFold:\".format(iKFold))\n",
    "    drawConfusionMatrix(conv[0] , conv[1])\n",
    "    print()\n",
    "    iKFold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save best DWT accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAccuracy(scores_Dwt , history_Dwt , 'three_level2_DWT.csv' , True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save best DWT& PCA accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAccuracy(scores_dwtPca , history_dwtPca , 'three_level2_PCA&DWT.csv' , True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save best DWT& MFCC accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAccuracy(scores_DwtMfcc , history_DwtMfcc , 'three_level2_DWT&MFCC.csv' , True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save best MFCC& PCA accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAccuracy(scores_mfccPca , history_mfccPca , 'three_level2_PCA&MFCC.csv' , True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
